ID,Type,Story,Parent,Points,Comments,Author,Title,URL,Content,Created
25080,comment,25079,25079,0,0,jamiequint,,,"I wrote this, let me know what you think.",1180381916
25079,story,,,5,1,jamiequint,"Startups and ""The Curse of Knowledge""",http://blog.jamiequint.com/2007/02/07/startups-and-the-curse-of-knowledge/,,1180381886
25078,comment,25042,25042,0,0,dhouston,,,"it depends on where your bottlenecks are. if you're serving static content (e.g. an image hosting or photo sharing service), amazon S3 is a cheap and simple way to do so. if you have a big database involved anywhere (e.g. facebook, which has to keep track of user profiles, mappings between users, groups, and god knows how many hundreds of other kinds of connections), it's a lot tougher.<p>in general, you can ""scale up"" (buy bigger servers) for a little while, but eventually you'll have to ""scale out"" (divide the load across a lot of smaller servers.) amazon's EC2 service lets you do this relatively easily. it's hardly plug and play though; you have to figure out how to scale out within your app and ensure that you can split up your data across multiple servers, which is often not trivial. for example, even if you can partition your data, what if one ""piece"" gets hammered with a disproportionate amount of traffic (i.e. one particular photo album or whatever gets dugg/slashdotted); what if a server goes down; what if you need to dynamically add servers; how do you protect data if a disk completely fails; etc.<p>here's some food for thought from the sysadmins of livejournal and digg, and how they dealt with growth.<p><a href=""http://www.getdropbox.com/u/2/Interesting/servers%2C%20infrastructure/danga.com%20livejournal%20backend.pdf"" rel=""nofollow"">http://www.getdropbox.com/u/2/Interesting/servers%2C%20infrastructure/danga.com%20livejournal%20backend.pdf</a>
<a href=""http://www.getdropbox.com/u/2/Interesting/servers%2C%20infrastructure/digg%20sysadmin%20presentation.pdf"" rel=""nofollow"">http://www.getdropbox.com/u/2/Interesting/servers%2C%20infrastructure/digg%20sysadmin%20presentation.pdf</a><p>perhaps better than the technical solutions are social solutions to hypergrowth: you can use invites, beta periods or waiting lists to control your growth instead of turning people away indefinitely, and also set users' expectations that things may go wrong and to bear with you. it's really tough to anticipate on the technical side where things will break if you're adding hundreds of k users; just look at ilike, which got rocked by f8 signups.<p>there are more pieces too besides just serving the http requests properly :P presumably you'll need to provide some support to these people, answer emails, fix things when they go wrong, have some method of discovering and triaging problems, etc.",1180381234
25077,comment,25029,25029,0,0,palish,,,"""Instead of 'programmers' (people that specialize in writing code), what you need are 'developers' (people who will contribute in multiple ways to make the product successful).""<p><a href=""http://www.ericsink.com/No_Programmers.html"" rel=""nofollow"">http://www.ericsink.com/No_Programmers.html</a>",1180381206
