ID,Type,Story,Parent,Points,Comments,Author,Title,URL,Content,Created
36749,story,,,2,0,bootload,"Twitter, month 5",http://www.scripting.com/stories/2007/07/25/twitterMonth5.html,,1185407993
36748,comment,36603,36665,0,0,bls,,,"I understand what you mean. But, who is going to be entering valuable, unreproducible data into Facebook? If they re-install your app then they will probably be pretty happy to enter everything all over again.",1185407248
36746,comment,36603,36603,0,0,bluishgreen,,,"Facebook is reminding me more and more of Microsoft, it would be good for them if they remind us of Google or something.\n[ microsoft = bad. google = ok.] ",1185405973
36745,comment,36706,36706,0,0,bls,,,"""Are you sick of emails that say 'thanks' or 'sounds great' and other pointless ones that you really didn't need to read?""<p>I never want to work with anybody that has that attitude. You don't have to time even READ (let alone politely respond to) a ""thank you?"" But, I bet you have plenty of time to read and respond to everything on digg, reddit, slashdot, news.yc, and facebook. ",1185405856
36744,comment,36507,36507,0,0,simpleenigma,,,"The only thing I can think of is to create a way to try to get the user to the page they were requesting as quickly as possible. Or at least make sure you keep them on your site instead of them wandering off somewhere else.<p>",1185405833
36743,comment,36440,36470,0,0,simpleenigma,,,"I completely agree, they do need to get that pioneering spirit back into the product line.<p>Although I think the movie studios are a bigger hurdle to digital delivery then almost any company can overcome. The sheer amount of DRM that they will need to place into any product they create will make it nearly useless and no one will make any money off of it because no one will want to use it.<p>So here is a good question: How do you solve the DRM vs consumer rights problem while still making money and not getting sued out of existence?<p>If Netflix answered this one question they would be more then relevant again.",1185405606
36742,comment,36704,36704,0,0,willarson,,,"I would take the first job. Many people without a complete formal education have this fear about the things they <i>should</i> know. They also assume that if they had only completed a CS major they would in fact understand these things. Unfortunately this doesn't play out.<p>\nI graduated with a CS major, and many of my classmates who graduated with the same did not have a  very strong grasp on ""algorithms, performance, or what makes beautiful code"". Don't let the lack of a formal CS education distress you. The students who know the most are the ones who are off doing their own projects and working with each other to create things, those who charge through the curriculum learn a lot, but don't internalize the knowledge, and thus retain little of it.",1185405547
36741,comment,36721,36721,0,0,glangley,,,"sorry let me in explain. i will have a database of locations and it already stores the address. i havent looked to deeply into the google maps api, so i am not sure if I will make just 1 request to google for the entire map or if it requires 50 requests to get every marker on the map.<p>so, would it make sense to cache the results? anyone have good links for tips on memcache?<p>thanks already for the great feedback\n",1185405422
36740,comment,36721,36721,0,0,willarson,,,"Epi0Bauqu is definitely correct that you do not want to access google 10-50 times with each page refresh, that would be very slow. <p>\nHowever, with the number of pieces of data you are requesting you'll have to be intelligent with your storage and querying mechanism. Example: you will very likely want to fork your queries to google to make the entire batch of requests simultaneously instead of sequentially. When you start implementing a caching/storage mechanism you'll want to keep this in mind too: accessing memcached 50 times sequentially will be slower than querying PostgreSQL once (in general, although if the database is very large and has to be paged in and out of memory, etc, this may not hold). So you'll probably want to figure out a way to store these requests in your database, and have a single query that can recall all the relevant geocodes (I don't know the details of how the geocodes will relate to each other, so I can't really be specific), and <i>then</i> you will want to cache that entire request. This will mean that you will (when dealing with data you have already retrieved from google... and prefetching data that you expect to see wouldn't be a bad idea) have at worst one SQL query, and at best a quick access in memcached.<p>\nIf you can't think of a way to group requests like this, I strongly suspect you will need to rethink your application. Performing 10-50 external requests per page refresh will put a serious damper on your site's performance, and making 50 database queries per page refresh or even 50 memcached queries per page refresh is probably untenable (although you could start caching the entire created pages, depending on what exactly your application does). To get sufficiently quick speed to do 50 queries, you'd probably have to use local memory (which is significantly quicker than memcached), but using local memory will open the door into a thread safety hell.",1185404716
36739,comment,36704,36704,0,0,Keios,,,"Job A if your want to improve your creativity.<p>Job B if you want to encourage your technique.<p>To improve my programming I generally do things that I can't figure out how to do in my head.",1185404559
