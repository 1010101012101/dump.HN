ID,Type,Story,Parent,Points,Comments,Author,Title,URL,Content,Created
76062,story,,,8,2,chaostheory,"Bye Bye PCs? PCs Being Pushed Aside in Japan by Array of Gadgets With Similar Power",http://biz.yahoo.com/ap/071104/bye_bye_pcs.html,,1194198849
76061,comment,76046,76046,0,0,raju,,,PDF!!!,1194198814
76060,story,,,1,0,DanielBMarkham,"Programming with Python -- the Other Guys",http://www.whattofix.com/blog/archives/2007/11/programming_wit.php,,1194198792
76059,comment,76007,76007,0,0,axod,,,"I really hope that if multiple cores increase, they are done in a way which hides them from the programmer.
For example a 32 core CPU, but that appears as a very very fast single core. That's where the concurrency/'threading' issues should live, not in everyones code.<p>Threads are usually the problem IMHO not the solution.<p>I don't agree with the suggestion that javascript will need threads either. Javascript works extremely well in a single thread. There isn't really much of a need for threads. Having multiple cores doesn't change that, it just means you might need some abstraction layer like I described above, that utilizes all cores, whilst appearing as a single core.",1194198091
76058,comment,76007,76023,0,0,Zak,,,"Unless somebody comes up with an app that's doing massive amounts of data processing in JS, I think giving each tab/window its own thread in the browser will be an adequate solution for the foreseeable future.",1194197991
76057,comment,76007,76042,0,0,tocomment,,,"Isn't stackless not truly concurrent since it's still beholden to the Python GIL?<p>Or is that not the case?",1194197615
76056,comment,75906,75930,0,0,nostrademons,,,"Go watch Kid Nation.  Wednesdays at 8:00 on CBS.",1194197496
76055,story,,,6,1,nickb,"Hack: Discover the hidden 8 bit Sound card in your PC",http://linuxgazette.net/109/john.html,,1194196798
76054,comment,76052,76052,0,0,nostrademons,,,"Damnit, I was gonna say ""finishing"". ;-)<p>Aside from that, it's the incremental refinement that happens after you have a basic architecture, a useful app that puts stuff up on the screen, and a small userbase.  Changes are usually pretty easy at that point (if you did the architecture right), and there's the instant gratification of writing a feature and having people immediately use it.<p>I find that most projects follow a ""ski jump"" happiness curve.  Thinking up the initial idea is a lot of fun, as are the first few planning stages.  Then you come up against all the little details that are absolutely necessary to build something useful, yet not really fun to deal with.  By the time you're about 2/3 through with building it, it looks hopeless.  Then you start cleaning things up and figuring out an architecture that reconciles all the different concerns.  If you haven't made <i>too</i> many tradeoffs by then (or skipped the step entirely), the launch and refinement process is kinda fun.",1194196683
76053,comment,76007,76007,0,0,Hexayurt,,,"Occam.<p>Specifically, <a href=""http://transterpreter.org"" rel=""nofollow"">http://transterpreter.org</a><p>Yes, the language it runs (Occam) is 20 years old. But the language was designed for programs running on dozens to thousands of nodes, and in the transterpreter implementation, there's the possibility of doing this on heterogeneous hardware, where the fast nodes do things like splitting and merging the data set, and the smaller ""grunt compute"" nodes do the actual work.<p>Parallel programming is hard, but that's inherent hardness. You can't get around things like memory bandwidth and latency at a programming language level, no matter how much you try. You can only get away from those things by dealing with the fact you have thousands of machines, or tens of thousands.<p>It's only going to get worse from here on in, as ""faster"" comes to mean more processors, not higher clock rates. You'll see this: 2 core! 3 core! 4 core! 8 core! and pretty soon (within 10 years) we'll see 64 and 128 core desktop machines, maybe even a revival of unusual architectures like wafter scale integration with 3D optical interconnects (i.e. upward pointing tiny lasers and photocells fabricated on the chip) to handle getting data on and off the processors.<p>We've seen unambiguously that <i></i>GIGANTIC<i></i> data sets have their own value. Google's optimization of their algorithms clearly uses enormous amounts of observed user behavior. Translation efforts with terabyte source cannons. Image integration algorithms like that thing that Microsoft were demonstrating recently... gigantic data sets have power because statistics draw relationships out of the real world, rather than having programmers guessing about what the relationships are.<p>I strongly suspect that 20 years from now, there are going to be three kinds of application programming:<p>1> Interface programming<p>2> Desktop programming (in the sense of programming things which operate on <i>your personal objects</i> - these things are like <i>pens and paper</i> and you have your own.)<p>3> Infrastructure programming - supercomputer cluster programming (Amazon and Google are <i>supercomputer</i> <i>applications</i> <i>companies</i>) - which will provide yer basic services.<p>One of the concepts I'm pitching to the military right now is using the massive data sets they have from satellite sources to provide ""precision agriculture"" support for the developing world. Precision Agriculture in America is tractors with GPS units that vary their fertilizer and pesticide distribution on a meter-by-meter basis (robotic valves consult the dataset as you drive around the land.)<p>In a developing world context, your farmers get the GPS coordinates for their land tied to their cell phone numbers either by an aid worker, or by their own cell phone company.<p>Then the USG runs code over their sat data, and comes up with farming recommendations for that plot of land. If the plots are small enough (and they often are) the entire plot is a single precision agriculture cell.<p>But if you think about the size of the datasets - we're talking about doing this for maybe 20 - 30% of the planet's landmass - and the software to interpret the images is non-trivial and only going to get more complex as modeling of crops and farming practices improves...<p>Real applications - change the world applications - need parallel supercomputer programming. Occam was <i>right</i> in the same way that Lisp is <i>right</i> but for a different class of problems. That's because Occam is CSP (concurrent sequential processes) and those are a Good Thing. There may need to be refinements to handle the fact we have much faster nodes, but much slower networks, than Occam was originally designed for - but that may also turn out to be a non-issue.<p>I'm also working on similar stuff around expert systems for primary health care - medical expert systems are already pretty well understood - so the notion is to develop an integrated set of medical practices (these 24 drugs which don't require refrigeration, don't produce overdose easily, and are less than $10 per course) with an expert system which can be accessed both by patients themselves to figure out if their symptoms are problematic or not, and by slightly trained health care workers who would use the systems to figure out what to prescribe from their standard pharmacopoeia.<p>It's not much, but for the poorest two or three billion, this could be the only health care service they ever see. None of the problems are particularly intractable, but you better bet there's a VAST - and I mean VAST - distributed call center application at the core of this.<p>Of course, the Right Way to do this is FOLDING@HOME or SETI - we've already proven that public interest supercomputing on a heterogeneous distributed network works.<p>Now we just need to turn it to something directly lifesaving, rather than indirectly important for broader reasons.<p>Remember that the richest 50% of the human race have cell phones already, and rumor has it (i.e. I read it on the internet) that phone numbers and internet users in Africa have doubled every year for the past seven years. 10 years from now the network is going to be ubiquitous, even among many of the very, very poorest.<p>We get a do-over here in our relationship with the developing world. We can't fix farm subsidies, but we can ensure that when they plug into the network for the first time, there is something useful there.
",1194196629
76052,story,,,5,11,matth,"Ask YC: For the hackers, what's your favorite part of development?",,"... aside from finishing, of course. Thinking up the original idea, planning, building, refactoring, showing your project off, etc. For me, it's definitely refactoring.",1194195732
